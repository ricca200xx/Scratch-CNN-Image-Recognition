{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d643ccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:31.478750Z",
     "iopub.status.busy": "2025-04-30T17:19:31.478538Z",
     "iopub.status.idle": "2025-04-30T17:19:45.223581Z",
     "shell.execute_reply": "2025-04-30T17:19:45.222292Z"
    },
    "papermill": {
     "duration": 13.751169,
     "end_time": "2025-04-30T17:19:45.225842",
     "exception": false,
     "start_time": "2025-04-30T17:19:31.474673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import os.path as osp\n",
    "import zipfile\n",
    "import csv\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import time\n",
    "import random\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, WeightedRandomSampler, Sampler \n",
    "from torchvision import transforms as T \n",
    "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingWarmRestarts\n",
    "from torch.amp import autocast, GradScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a15554f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:45.237094Z",
     "iopub.status.busy": "2025-04-30T17:19:45.236128Z",
     "iopub.status.idle": "2025-04-30T17:19:45.241274Z",
     "shell.execute_reply": "2025-04-30T17:19:45.240663Z"
    },
    "papermill": {
     "duration": 0.010984,
     "end_time": "2025-04-30T17:19:45.242502",
     "exception": false,
     "start_time": "2025-04-30T17:19:45.231518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dir = \"/kaggle/input/unipd-deep-learning-2025-challenge-1/train_dataset\"\n",
    "test_dir  = \"/kaggle/input/unipd-deep-learning-2025-challenge-1/test_dataset\"\n",
    "\n",
    "\n",
    "mean = [0.5131654143333435, 0.46465885639190674, 0.4044109880924225]\n",
    "std = [0.2625938653945923, 0.25588247179985046, 0.27475273609161377]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17d446c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:45.248664Z",
     "iopub.status.busy": "2025-04-30T17:19:45.248297Z",
     "iopub.status.idle": "2025-04-30T17:19:45.261376Z",
     "shell.execute_reply": "2025-04-30T17:19:45.260863Z"
    },
    "papermill": {
     "duration": 0.017348,
     "end_time": "2025-04-30T17:19:45.262320",
     "exception": false,
     "start_time": "2025-04-30T17:19:45.244972",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root: str, test: bool=False, transform=None):\n",
    "        super().__init__()\n",
    "        self.root = root\n",
    "        self.test = test\n",
    "        self.transform = transform\n",
    "        if osp.exists(osp.join(root,'images')):\n",
    "             self.img_path = osp.join(root,'images')\n",
    "        elif osp.exists(osp.join(root, osp.basename(root))): \n",
    "             self.img_path = osp.join(root, osp.basename(root))\n",
    "        else:\n",
    "             self.img_path = root\n",
    "\n",
    "        self.ids, self.targets = [], []\n",
    "\n",
    "        if not test:\n",
    "            label_file = osp.join(root,'labels.csv')\n",
    "            if not osp.exists(label_file) and osp.exists(osp.join(osp.dirname(root), 'labels.csv')):\n",
    "                 label_file = osp.join(osp.dirname(root), 'labels.csv')\n",
    "\n",
    "            with open(label_file) as f:\n",
    "                reader = csv.DictReader(f)\n",
    "                for r in reader:\n",
    "                    self.ids.append(r['id'].zfill(5))\n",
    "                    self.targets.append(int(r['label']))\n",
    "        else:\n",
    "            image_files = [fn for fn in sorted(os.listdir(self.img_path)) if fn.lower().endswith('.jpeg')]\n",
    "            print(f\"Found {len(image_files)} test images in {self.img_path}\")\n",
    "            for fn in image_files:\n",
    "                 self.ids.append(osp.splitext(fn)[0])\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        img = Image.open(osp.join(self.img_path,f\"{img_id}.jpeg\")).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return (img, img_id) if self.test else (img, self.targets[idx])\n",
    "\n",
    "class BalancedSampler(Sampler):\n",
    "    def __init__(self, dataset, target_count):\n",
    "        self.dataset = dataset\n",
    "        self.target_count = target_count\n",
    "\n",
    "        if hasattr(dataset, 'indices') and hasattr(dataset.dataset, 'targets'):\n",
    "            original_targets = np.array(dataset.dataset.targets)\n",
    "            self.labels = original_targets[dataset.indices].tolist()\n",
    "        else: \n",
    "            self.labels = [dataset[i][1] for i in range(len(dataset))]\n",
    "\n",
    "\n",
    "        self.class_indices = {}\n",
    "        for subset_idx, label in enumerate(self.labels):\n",
    "            self.class_indices.setdefault(label, []).append(subset_idx) \n",
    "\n",
    "        self.balanced_indices = []\n",
    "        for label, indices in self.class_indices.items():\n",
    "            current_count = len(indices)\n",
    "\n",
    "            if current_count == 0: continue \n",
    "\n",
    "            if current_count < target_count:\n",
    "                oversample_factor = target_count // current_count\n",
    "                remainder = target_count % current_count\n",
    "                self.balanced_indices.extend(indices * oversample_factor)\n",
    "                self.balanced_indices.extend(random.sample(indices, k=remainder))\n",
    "            else:\n",
    "                self.balanced_indices.extend(random.sample(indices, k=target_count))\n",
    "\n",
    "        random.shuffle(self.balanced_indices)\n",
    "        print(f\"BalancedSampler: Created {len(self.balanced_indices)} indices targeting {target_count} per class.\")\n",
    "\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.balanced_indices)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.balanced_indices)\n",
    "\n",
    "class TransformedSubset(Dataset):\n",
    "    def __init__(self, subset, transform=None):\n",
    "        self.subset = subset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = self.subset[index] \n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf985ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:45.268237Z",
     "iopub.status.busy": "2025-04-30T17:19:45.268017Z",
     "iopub.status.idle": "2025-04-30T17:19:45.324603Z",
     "shell.execute_reply": "2025-04-30T17:19:45.323748Z"
    },
    "papermill": {
     "duration": 0.061066,
     "end_time": "2025-04-30T17:19:45.325886",
     "exception": false,
     "start_time": "2025-04-30T17:19:45.264820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes detected: 20\n"
     ]
    }
   ],
   "source": [
    "INPUT_SIZE = 96\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((INPUT_SIZE + 30, INPUT_SIZE + 30)),\n",
    "    T.RandomResizedCrop(INPUT_SIZE, scale=(0.5, 1.0)),  \n",
    "    T.RandomHorizontalFlip(p=0.6),\n",
    "    T.RandomVerticalFlip(p=0.2),      \n",
    "    T.RandomRotation(30),\n",
    "    T.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std)\n",
    "])\n",
    "\n",
    "\n",
    "val_transform = T.Compose([\n",
    "    T.Resize((INPUT_SIZE, INPUT_SIZE)), \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std)])\n",
    "\n",
    "base_dataset = ImageDataset(root=train_dir, transform=None)\n",
    "\n",
    "NUM_CLASSES = len(set(base_dataset.targets))\n",
    "print(f\"Number of classes detected: {NUM_CLASSES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a009d47b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:45.332270Z",
     "iopub.status.busy": "2025-04-30T17:19:45.331874Z",
     "iopub.status.idle": "2025-04-30T17:19:45.501403Z",
     "shell.execute_reply": "2025-04-30T17:19:45.500591Z"
    },
    "papermill": {
     "duration": 0.173812,
     "end_time": "2025-04-30T17:19:45.502635",
     "exception": false,
     "start_time": "2025-04-30T17:19:45.328823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution in full dataset: Counter({11: 1300, 10: 1300, 9: 1300, 3: 1300, 8: 1300, 1: 1300, 0: 1300, 4: 1300, 17: 1300, 14: 1300, 12: 1300, 6: 1300, 2: 1300, 18: 1300, 19: 760, 13: 756, 5: 755, 16: 751, 7: 658, 15: 550})\n",
      "Using target_count=1800 for BalancedSampler.\n",
      "BalancedSampler: Created 36000 indices targeting 1800 per class.\n",
      "Found 4000 test images in /kaggle/input/unipd-deep-learning-2025-challenge-1/test_dataset/images\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.80 * len(base_dataset)) \n",
    "val_size = len(base_dataset) - train_size\n",
    "train_subset, val_subset = random_split(base_dataset, [train_size, val_size])\n",
    "\n",
    "train_dataset_transformed = TransformedSubset(train_subset, transform=train_transform)\n",
    "val_dataset_transformed = TransformedSubset(val_subset, transform=val_transform)\n",
    "label_counts = Counter(base_dataset.targets)\n",
    "print(\"Class distribution in full dataset:\", label_counts)\n",
    "\n",
    "target_count_sampler = 1800 \n",
    "print(f\"Using target_count={target_count_sampler} for BalancedSampler.\")\n",
    "train_sampler = BalancedSampler(train_subset, target_count=target_count_sampler)\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset_transformed,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=train_sampler, \n",
    "    num_workers=2,\n",
    "    pin_memory=True, \n",
    "    drop_last=True \n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset_transformed,\n",
    "    batch_size=BATCH_SIZE * 2, \n",
    "    shuffle=False, \n",
    "    num_workers=2,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "test_ds = ImageDataset(test_dir, test=True, transform=val_transform) \n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE * 2, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f09c45b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:45.510198Z",
     "iopub.status.busy": "2025-04-30T17:19:45.509712Z",
     "iopub.status.idle": "2025-04-30T17:19:45.516616Z",
     "shell.execute_reply": "2025-04-30T17:19:45.515906Z"
    },
    "papermill": {
     "duration": 0.012652,
     "end_time": "2025-04-30T17:19:45.517933",
     "exception": false,
     "start_time": "2025-04-30T17:19:45.505281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImagerClassifier(nn.Module):\n",
    "    def __init__(self, num_classes, dropout_rate=0.2):  \n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2),\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.GELU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            \n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63de49d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:45.523987Z",
     "iopub.status.busy": "2025-04-30T17:19:45.523760Z",
     "iopub.status.idle": "2025-04-30T17:19:45.535677Z",
     "shell.execute_reply": "2025-04-30T17:19:45.534962Z"
    },
    "papermill": {
     "duration": 0.016297,
     "end_time": "2025-04-30T17:19:45.536783",
     "exception": false,
     "start_time": "2025-04-30T17:19:45.520486",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, scheduler, num_epochs, device,\n",
    "                patience=10, best_model_path='best_model.pth', use_amp=True):\n",
    "\n",
    "    model.to(device)\n",
    "    scaler = GradScaler(enabled=use_amp) \n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    best_model_weights = copy.deepcopy(model.state_dict()) \n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    total_steps = len(train_loader) * num_epochs \n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_predictions = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast(device_type=device.type, enabled=use_amp):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            if isinstance(scheduler, OneCycleLR):\n",
    "                 scheduler.step() \n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_samples += labels.size(0)\n",
    "            correct_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_train_loss = running_loss / total_samples\n",
    "        epoch_train_acc = correct_predictions / total_samples\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "\n",
    "        #Validation\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val_predictions = 0\n",
    "        total_val_samples = 0\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                with autocast(device_type=device.type, enabled=use_amp):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                running_val_loss += loss.item() * inputs.size(0)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total_val_samples += labels.size(0)\n",
    "                correct_val_predictions += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = running_val_loss / total_val_samples\n",
    "        epoch_val_acc = correct_val_predictions / total_val_samples\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f} - \"\n",
    "              f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f} - \"\n",
    "              f\"LR: {optimizer.param_groups[0]['lr']:.6f}\") \n",
    "\n",
    "        #Early Stopping\n",
    "        if epoch_val_acc > best_val_accuracy:\n",
    "            print(f\"Validation accuracy improved ({best_val_accuracy:.4f} --> {epoch_val_acc:.4f}). Saving model...\")\n",
    "            best_val_accuracy = epoch_val_acc\n",
    "            best_model_weights = copy.deepcopy(model.state_dict())\n",
    "            torch.save(best_model_weights, best_model_path)\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            print(f\"Validation accuracy did not improve for {epochs_no_improve} epoch(s).\")\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Training finished in {(end_time - start_time)/60:.2f} minutes.\")\n",
    "    print(f\"Best validation accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "    model.load_state_dict(torch.load(best_model_path)) \n",
    "    return model, history \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3279a574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:45.542774Z",
     "iopub.status.busy": "2025-04-30T17:19:45.542221Z",
     "iopub.status.idle": "2025-04-30T17:19:45.871319Z",
     "shell.execute_reply": "2025-04-30T17:19:45.870749Z"
    },
    "papermill": {
     "duration": 0.333304,
     "end_time": "2025-04-30T17:19:45.872587",
     "exception": false,
     "start_time": "2025-04-30T17:19:45.539283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "labels = base_dataset.targets\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5933c0cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T17:19:45.878715Z",
     "iopub.status.busy": "2025-04-30T17:19:45.878513Z",
     "iopub.status.idle": "2025-04-30T19:21:14.977829Z",
     "shell.execute_reply": "2025-04-30T19:21:14.976968Z"
    },
    "papermill": {
     "duration": 7289.109816,
     "end_time": "2025-04-30T19:21:14.985228",
     "exception": false,
     "start_time": "2025-04-30T17:19:45.875412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100] - Train Loss: 2.5826, Train Acc: 0.1906 - Val Loss: 2.4650, Val Acc: 0.2218 - LR: 0.000051\n",
      "Validation accuracy improved (0.0000 --> 0.2218). Saving model...\n",
      "Epoch [2/100] - Train Loss: 2.2868, Train Acc: 0.2859 - Val Loss: 2.2815, Val Acc: 0.2922 - LR: 0.000070\n",
      "Validation accuracy improved (0.2218 --> 0.2922). Saving model...\n",
      "Epoch [3/100] - Train Loss: 2.1231, Train Acc: 0.3390 - Val Loss: 2.2892, Val Acc: 0.3304 - LR: 0.000090\n",
      "Validation accuracy improved (0.2922 --> 0.3304). Saving model...\n",
      "Epoch [4/100] - Train Loss: 2.0308, Train Acc: 0.3728 - Val Loss: 2.1176, Val Acc: 0.3819 - LR: 0.000109\n",
      "Validation accuracy improved (0.3304 --> 0.3819). Saving model...\n",
      "Epoch [5/100] - Train Loss: 1.9672, Train Acc: 0.3976 - Val Loss: 2.1080, Val Acc: 0.3816 - LR: 0.000128\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [6/100] - Train Loss: 1.8980, Train Acc: 0.4317 - Val Loss: 2.2780, Val Acc: 0.3437 - LR: 0.000147\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [7/100] - Train Loss: 1.8478, Train Acc: 0.4537 - Val Loss: 1.9474, Val Acc: 0.4376 - LR: 0.000166\n",
      "Validation accuracy improved (0.3819 --> 0.4376). Saving model...\n",
      "Epoch [8/100] - Train Loss: 1.8000, Train Acc: 0.4712 - Val Loss: 2.0130, Val Acc: 0.4336 - LR: 0.000186\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [9/100] - Train Loss: 1.7680, Train Acc: 0.4884 - Val Loss: 2.0719, Val Acc: 0.4148 - LR: 0.000205\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [10/100] - Train Loss: 1.7362, Train Acc: 0.5025 - Val Loss: 1.8998, Val Acc: 0.4724 - LR: 0.000224\n",
      "Validation accuracy improved (0.4376 --> 0.4724). Saving model...\n",
      "Epoch [11/100] - Train Loss: 1.7096, Train Acc: 0.5136 - Val Loss: 1.9996, Val Acc: 0.4409 - LR: 0.000243\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [12/100] - Train Loss: 1.6792, Train Acc: 0.5282 - Val Loss: 1.6909, Val Acc: 0.5526 - LR: 0.000262\n",
      "Validation accuracy improved (0.4724 --> 0.5526). Saving model...\n",
      "Epoch [13/100] - Train Loss: 1.6516, Train Acc: 0.5420 - Val Loss: 1.9244, Val Acc: 0.4712 - LR: 0.000282\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [14/100] - Train Loss: 1.6306, Train Acc: 0.5507 - Val Loss: 2.0135, Val Acc: 0.4420 - LR: 0.000301\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [15/100] - Train Loss: 1.6089, Train Acc: 0.5584 - Val Loss: 2.0528, Val Acc: 0.4496 - LR: 0.000320\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [16/100] - Train Loss: 1.5906, Train Acc: 0.5692 - Val Loss: 1.7167, Val Acc: 0.5571 - LR: 0.000339\n",
      "Validation accuracy improved (0.5526 --> 0.5571). Saving model...\n",
      "Epoch [17/100] - Train Loss: 1.5780, Train Acc: 0.5736 - Val Loss: 1.7107, Val Acc: 0.5597 - LR: 0.000358\n",
      "Validation accuracy improved (0.5571 --> 0.5597). Saving model...\n",
      "Epoch [18/100] - Train Loss: 1.5580, Train Acc: 0.5799 - Val Loss: 1.8168, Val Acc: 0.5116 - LR: 0.000378\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [19/100] - Train Loss: 1.5388, Train Acc: 0.5940 - Val Loss: 1.6332, Val Acc: 0.5753 - LR: 0.000397\n",
      "Validation accuracy improved (0.5597 --> 0.5753). Saving model...\n",
      "Epoch [20/100] - Train Loss: 1.5261, Train Acc: 0.5986 - Val Loss: 1.6319, Val Acc: 0.5849 - LR: 0.000416\n",
      "Validation accuracy improved (0.5753 --> 0.5849). Saving model...\n",
      "Epoch [21/100] - Train Loss: 1.5125, Train Acc: 0.6038 - Val Loss: 1.7448, Val Acc: 0.5401 - LR: 0.000435\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [22/100] - Train Loss: 1.4901, Train Acc: 0.6147 - Val Loss: 1.6098, Val Acc: 0.5901 - LR: 0.000454\n",
      "Validation accuracy improved (0.5849 --> 0.5901). Saving model...\n",
      "Epoch [23/100] - Train Loss: 1.4801, Train Acc: 0.6157 - Val Loss: 1.6496, Val Acc: 0.5865 - LR: 0.000474\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [24/100] - Train Loss: 1.4733, Train Acc: 0.6237 - Val Loss: 1.7416, Val Acc: 0.5600 - LR: 0.000493\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [25/100] - Train Loss: 1.4580, Train Acc: 0.6275 - Val Loss: 1.5971, Val Acc: 0.6086 - LR: 0.000512\n",
      "Validation accuracy improved (0.5901 --> 0.6086). Saving model...\n",
      "Epoch [26/100] - Train Loss: 1.4394, Train Acc: 0.6327 - Val Loss: 1.7673, Val Acc: 0.5430 - LR: 0.000531\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [27/100] - Train Loss: 1.4315, Train Acc: 0.6383 - Val Loss: 1.6645, Val Acc: 0.5831 - LR: 0.000550\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [28/100] - Train Loss: 1.4185, Train Acc: 0.6449 - Val Loss: 1.6000, Val Acc: 0.6010 - LR: 0.000570\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [29/100] - Train Loss: 1.4049, Train Acc: 0.6528 - Val Loss: 1.5864, Val Acc: 0.6128 - LR: 0.000589\n",
      "Validation accuracy improved (0.6086 --> 0.6128). Saving model...\n",
      "Epoch [30/100] - Train Loss: 1.3978, Train Acc: 0.6565 - Val Loss: 1.5858, Val Acc: 0.6083 - LR: 0.000608\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [31/100] - Train Loss: 1.3879, Train Acc: 0.6601 - Val Loss: 1.5719, Val Acc: 0.6166 - LR: 0.000627\n",
      "Validation accuracy improved (0.6128 --> 0.6166). Saving model...\n",
      "Epoch [32/100] - Train Loss: 1.3750, Train Acc: 0.6658 - Val Loss: 1.5406, Val Acc: 0.6337 - LR: 0.000646\n",
      "Validation accuracy improved (0.6166 --> 0.6337). Saving model...\n",
      "Epoch [33/100] - Train Loss: 1.3650, Train Acc: 0.6682 - Val Loss: 1.5006, Val Acc: 0.6460 - LR: 0.000666\n",
      "Validation accuracy improved (0.6337 --> 0.6460). Saving model...\n",
      "Epoch [34/100] - Train Loss: 1.3495, Train Acc: 0.6758 - Val Loss: 1.4854, Val Acc: 0.6523 - LR: 0.000685\n",
      "Validation accuracy improved (0.6460 --> 0.6523). Saving model...\n",
      "Epoch [35/100] - Train Loss: 1.3479, Train Acc: 0.6792 - Val Loss: 1.4639, Val Acc: 0.6589 - LR: 0.000704\n",
      "Validation accuracy improved (0.6523 --> 0.6589). Saving model...\n",
      "Epoch [36/100] - Train Loss: 1.3383, Train Acc: 0.6819 - Val Loss: 1.5132, Val Acc: 0.6422 - LR: 0.000723\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [37/100] - Train Loss: 1.3243, Train Acc: 0.6888 - Val Loss: 1.5161, Val Acc: 0.6400 - LR: 0.000742\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [38/100] - Train Loss: 1.3197, Train Acc: 0.6896 - Val Loss: 1.5673, Val Acc: 0.6248 - LR: 0.000762\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [39/100] - Train Loss: 1.3095, Train Acc: 0.6929 - Val Loss: 1.4904, Val Acc: 0.6534 - LR: 0.000781\n",
      "Validation accuracy did not improve for 4 epoch(s).\n",
      "Epoch [40/100] - Train Loss: 1.2952, Train Acc: 0.7002 - Val Loss: 1.4644, Val Acc: 0.6710 - LR: 0.000800\n",
      "Validation accuracy improved (0.6589 --> 0.6710). Saving model...\n",
      "Epoch [41/100] - Train Loss: 1.2921, Train Acc: 0.7026 - Val Loss: 1.4722, Val Acc: 0.6596 - LR: 0.000787\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [42/100] - Train Loss: 1.2735, Train Acc: 0.7121 - Val Loss: 1.4970, Val Acc: 0.6509 - LR: 0.000773\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [43/100] - Train Loss: 1.2609, Train Acc: 0.7189 - Val Loss: 1.4726, Val Acc: 0.6638 - LR: 0.000760\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [44/100] - Train Loss: 1.2485, Train Acc: 0.7210 - Val Loss: 1.4376, Val Acc: 0.6763 - LR: 0.000747\n",
      "Validation accuracy improved (0.6710 --> 0.6763). Saving model...\n",
      "Epoch [45/100] - Train Loss: 1.2334, Train Acc: 0.7296 - Val Loss: 1.4272, Val Acc: 0.6857 - LR: 0.000733\n",
      "Validation accuracy improved (0.6763 --> 0.6857). Saving model...\n",
      "Epoch [46/100] - Train Loss: 1.2254, Train Acc: 0.7369 - Val Loss: 1.4415, Val Acc: 0.6801 - LR: 0.000720\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [47/100] - Train Loss: 1.2155, Train Acc: 0.7404 - Val Loss: 1.5267, Val Acc: 0.6487 - LR: 0.000707\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [48/100] - Train Loss: 1.2064, Train Acc: 0.7427 - Val Loss: 1.3773, Val Acc: 0.7006 - LR: 0.000693\n",
      "Validation accuracy improved (0.6857 --> 0.7006). Saving model...\n",
      "Epoch [49/100] - Train Loss: 1.1907, Train Acc: 0.7497 - Val Loss: 1.4494, Val Acc: 0.6730 - LR: 0.000680\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [50/100] - Train Loss: 1.1799, Train Acc: 0.7540 - Val Loss: 1.3842, Val Acc: 0.7037 - LR: 0.000667\n",
      "Validation accuracy improved (0.7006 --> 0.7037). Saving model...\n",
      "Epoch [51/100] - Train Loss: 1.1753, Train Acc: 0.7565 - Val Loss: 1.3892, Val Acc: 0.6991 - LR: 0.000653\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [52/100] - Train Loss: 1.1642, Train Acc: 0.7621 - Val Loss: 1.4315, Val Acc: 0.6848 - LR: 0.000640\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [53/100] - Train Loss: 1.1525, Train Acc: 0.7670 - Val Loss: 1.4043, Val Acc: 0.7011 - LR: 0.000627\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [54/100] - Train Loss: 1.1439, Train Acc: 0.7726 - Val Loss: 1.4029, Val Acc: 0.6977 - LR: 0.000613\n",
      "Validation accuracy did not improve for 4 epoch(s).\n",
      "Epoch [55/100] - Train Loss: 1.1344, Train Acc: 0.7756 - Val Loss: 1.4615, Val Acc: 0.6765 - LR: 0.000600\n",
      "Validation accuracy did not improve for 5 epoch(s).\n",
      "Epoch [56/100] - Train Loss: 1.1290, Train Acc: 0.7789 - Val Loss: 1.4370, Val Acc: 0.6801 - LR: 0.000587\n",
      "Validation accuracy did not improve for 6 epoch(s).\n",
      "Epoch [57/100] - Train Loss: 1.1172, Train Acc: 0.7858 - Val Loss: 1.4180, Val Acc: 0.6935 - LR: 0.000573\n",
      "Validation accuracy did not improve for 7 epoch(s).\n",
      "Epoch [58/100] - Train Loss: 1.1115, Train Acc: 0.7878 - Val Loss: 1.3545, Val Acc: 0.7091 - LR: 0.000560\n",
      "Validation accuracy improved (0.7037 --> 0.7091). Saving model...\n",
      "Epoch [59/100] - Train Loss: 1.1014, Train Acc: 0.7932 - Val Loss: 1.4174, Val Acc: 0.6926 - LR: 0.000547\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [60/100] - Train Loss: 1.0972, Train Acc: 0.7955 - Val Loss: 1.4285, Val Acc: 0.6875 - LR: 0.000533\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [61/100] - Train Loss: 1.0866, Train Acc: 0.7997 - Val Loss: 1.4001, Val Acc: 0.7040 - LR: 0.000520\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [62/100] - Train Loss: 1.0805, Train Acc: 0.8042 - Val Loss: 1.3489, Val Acc: 0.7216 - LR: 0.000507\n",
      "Validation accuracy improved (0.7091 --> 0.7216). Saving model...\n",
      "Epoch [63/100] - Train Loss: 1.0761, Train Acc: 0.8065 - Val Loss: 1.3683, Val Acc: 0.7173 - LR: 0.000493\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [64/100] - Train Loss: 1.0665, Train Acc: 0.8108 - Val Loss: 1.3921, Val Acc: 0.7069 - LR: 0.000480\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [65/100] - Train Loss: 1.0637, Train Acc: 0.8098 - Val Loss: 1.4147, Val Acc: 0.6962 - LR: 0.000467\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [66/100] - Train Loss: 1.0515, Train Acc: 0.8195 - Val Loss: 1.3490, Val Acc: 0.7151 - LR: 0.000453\n",
      "Validation accuracy did not improve for 4 epoch(s).\n",
      "Epoch [67/100] - Train Loss: 1.0526, Train Acc: 0.8164 - Val Loss: 1.3643, Val Acc: 0.7144 - LR: 0.000440\n",
      "Validation accuracy did not improve for 5 epoch(s).\n",
      "Epoch [68/100] - Train Loss: 1.0413, Train Acc: 0.8233 - Val Loss: 1.3713, Val Acc: 0.7138 - LR: 0.000427\n",
      "Validation accuracy did not improve for 6 epoch(s).\n",
      "Epoch [69/100] - Train Loss: 1.0328, Train Acc: 0.8293 - Val Loss: 1.3610, Val Acc: 0.7071 - LR: 0.000413\n",
      "Validation accuracy did not improve for 7 epoch(s).\n",
      "Epoch [70/100] - Train Loss: 1.0328, Train Acc: 0.8272 - Val Loss: 1.3283, Val Acc: 0.7327 - LR: 0.000400\n",
      "Validation accuracy improved (0.7216 --> 0.7327). Saving model...\n",
      "Epoch [71/100] - Train Loss: 1.0171, Train Acc: 0.8352 - Val Loss: 1.3630, Val Acc: 0.7140 - LR: 0.000387\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [72/100] - Train Loss: 1.0180, Train Acc: 0.8367 - Val Loss: 1.3522, Val Acc: 0.7265 - LR: 0.000373\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [73/100] - Train Loss: 1.0113, Train Acc: 0.8380 - Val Loss: 1.3507, Val Acc: 0.7191 - LR: 0.000360\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [74/100] - Train Loss: 1.0050, Train Acc: 0.8417 - Val Loss: 1.3481, Val Acc: 0.7245 - LR: 0.000347\n",
      "Validation accuracy did not improve for 4 epoch(s).\n",
      "Epoch [75/100] - Train Loss: 1.0008, Train Acc: 0.8424 - Val Loss: 1.3254, Val Acc: 0.7374 - LR: 0.000333\n",
      "Validation accuracy improved (0.7327 --> 0.7374). Saving model...\n",
      "Epoch [76/100] - Train Loss: 0.9922, Train Acc: 0.8497 - Val Loss: 1.3429, Val Acc: 0.7265 - LR: 0.000320\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [77/100] - Train Loss: 0.9916, Train Acc: 0.8492 - Val Loss: 1.3549, Val Acc: 0.7198 - LR: 0.000307\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [78/100] - Train Loss: 0.9868, Train Acc: 0.8516 - Val Loss: 1.3330, Val Acc: 0.7276 - LR: 0.000293\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [79/100] - Train Loss: 0.9810, Train Acc: 0.8540 - Val Loss: 1.3512, Val Acc: 0.7211 - LR: 0.000280\n",
      "Validation accuracy did not improve for 4 epoch(s).\n",
      "Epoch [80/100] - Train Loss: 0.9741, Train Acc: 0.8600 - Val Loss: 1.3566, Val Acc: 0.7205 - LR: 0.000267\n",
      "Validation accuracy did not improve for 5 epoch(s).\n",
      "Epoch [81/100] - Train Loss: 0.9692, Train Acc: 0.8593 - Val Loss: 1.3454, Val Acc: 0.7272 - LR: 0.000253\n",
      "Validation accuracy did not improve for 6 epoch(s).\n",
      "Epoch [82/100] - Train Loss: 0.9654, Train Acc: 0.8634 - Val Loss: 1.3433, Val Acc: 0.7280 - LR: 0.000240\n",
      "Validation accuracy did not improve for 7 epoch(s).\n",
      "Epoch [83/100] - Train Loss: 0.9632, Train Acc: 0.8632 - Val Loss: 1.3423, Val Acc: 0.7321 - LR: 0.000227\n",
      "Validation accuracy did not improve for 8 epoch(s).\n",
      "Epoch [84/100] - Train Loss: 0.9551, Train Acc: 0.8691 - Val Loss: 1.3358, Val Acc: 0.7305 - LR: 0.000213\n",
      "Validation accuracy did not improve for 9 epoch(s).\n",
      "Epoch [85/100] - Train Loss: 0.9541, Train Acc: 0.8684 - Val Loss: 1.3377, Val Acc: 0.7269 - LR: 0.000200\n",
      "Validation accuracy did not improve for 10 epoch(s).\n",
      "Epoch [86/100] - Train Loss: 0.9487, Train Acc: 0.8704 - Val Loss: 1.3282, Val Acc: 0.7381 - LR: 0.000187\n",
      "Validation accuracy improved (0.7374 --> 0.7381). Saving model...\n",
      "Epoch [87/100] - Train Loss: 0.9457, Train Acc: 0.8718 - Val Loss: 1.3287, Val Acc: 0.7376 - LR: 0.000173\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [88/100] - Train Loss: 0.9429, Train Acc: 0.8742 - Val Loss: 1.3255, Val Acc: 0.7309 - LR: 0.000160\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [89/100] - Train Loss: 0.9323, Train Acc: 0.8796 - Val Loss: 1.3176, Val Acc: 0.7443 - LR: 0.000147\n",
      "Validation accuracy improved (0.7381 --> 0.7443). Saving model...\n",
      "Epoch [90/100] - Train Loss: 0.9349, Train Acc: 0.8795 - Val Loss: 1.3262, Val Acc: 0.7370 - LR: 0.000133\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [91/100] - Train Loss: 0.9313, Train Acc: 0.8807 - Val Loss: 1.3227, Val Acc: 0.7390 - LR: 0.000120\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Epoch [92/100] - Train Loss: 0.9275, Train Acc: 0.8819 - Val Loss: 1.3150, Val Acc: 0.7381 - LR: 0.000107\n",
      "Validation accuracy did not improve for 3 epoch(s).\n",
      "Epoch [93/100] - Train Loss: 0.9253, Train Acc: 0.8839 - Val Loss: 1.3181, Val Acc: 0.7443 - LR: 0.000093\n",
      "Validation accuracy did not improve for 4 epoch(s).\n",
      "Epoch [94/100] - Train Loss: 0.9191, Train Acc: 0.8876 - Val Loss: 1.3114, Val Acc: 0.7428 - LR: 0.000080\n",
      "Validation accuracy did not improve for 5 epoch(s).\n",
      "Epoch [95/100] - Train Loss: 0.9145, Train Acc: 0.8894 - Val Loss: 1.3092, Val Acc: 0.7443 - LR: 0.000067\n",
      "Validation accuracy did not improve for 6 epoch(s).\n",
      "Epoch [96/100] - Train Loss: 0.9127, Train Acc: 0.8906 - Val Loss: 1.3153, Val Acc: 0.7407 - LR: 0.000053\n",
      "Validation accuracy did not improve for 7 epoch(s).\n",
      "Epoch [97/100] - Train Loss: 0.9143, Train Acc: 0.8902 - Val Loss: 1.3121, Val Acc: 0.7452 - LR: 0.000040\n",
      "Validation accuracy improved (0.7443 --> 0.7452). Saving model...\n",
      "Epoch [98/100] - Train Loss: 0.9091, Train Acc: 0.8940 - Val Loss: 1.3111, Val Acc: 0.7461 - LR: 0.000027\n",
      "Validation accuracy improved (0.7452 --> 0.7461). Saving model...\n",
      "Epoch [99/100] - Train Loss: 0.9049, Train Acc: 0.8955 - Val Loss: 1.3119, Val Acc: 0.7421 - LR: 0.000013\n",
      "Validation accuracy did not improve for 1 epoch(s).\n",
      "Epoch [100/100] - Train Loss: 0.9044, Train Acc: 0.8956 - Val Loss: 1.3111, Val Acc: 0.7401 - LR: -0.000000\n",
      "Validation accuracy did not improve for 2 epoch(s).\n",
      "Training finished in 121.48 minutes.\n",
      "Best validation accuracy: 0.7461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/3400116327.py:95: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_model_path))\n"
     ]
    }
   ],
   "source": [
    "LEARNING_RATE = 2e-4  \n",
    "WEIGHT_DECAY = 1e-4    \n",
    "EPOCHS = 100   \n",
    "PATIENCE = 20   \n",
    "BEST_MODEL_PATH = 'large_cnn_best_model.pth'\n",
    "USE_AMP = torch.cuda.is_available() \n",
    "\n",
    "model = ImagerClassifier(num_classes=NUM_CLASSES, dropout_rate=0.3).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights,label_smoothing=0.1)\n",
    "optimizer = torch.optim.AdamW(model.parameters(),lr=LEARNING_RATE,weight_decay=WEIGHT_DECAY,betas=(0.9, 0.999))\n",
    "scheduler = OneCycleLR(optimizer,max_lr=LEARNING_RATE*4,epochs=EPOCHS,steps_per_epoch=len(train_loader),pct_start=0.4,anneal_strategy='linear')\n",
    "best_model, history = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler, \n",
    "    num_epochs=EPOCHS,\n",
    "    device=DEVICE,\n",
    "    patience=PATIENCE,\n",
    "    best_model_path=BEST_MODEL_PATH,\n",
    "    use_amp=USE_AMP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c21a22",
   "metadata": {
    "papermill": {
     "duration": 0.0062,
     "end_time": "2025-04-30T19:21:14.997851",
     "exception": false,
     "start_time": "2025-04-30T19:21:14.991651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aba69b40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T19:21:15.012367Z",
     "iopub.status.busy": "2025-04-30T19:21:15.011746Z",
     "iopub.status.idle": "2025-04-30T19:21:19.073768Z",
     "shell.execute_reply": "2025-04-30T19:21:19.072697Z"
    },
    "papermill": {
     "duration": 4.070914,
     "end_time": "2025-04-30T19:21:19.075193",
     "exception": false,
     "start_time": "2025-04-30T19:21:15.004279",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validation Set Evaluation (Best Model):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5042    0.4651    0.4839       258\n",
      "           1     0.8233    0.7100    0.7625       269\n",
      "           2     0.8756    0.8678    0.8717       227\n",
      "           3     0.5639    0.5155    0.5386       291\n",
      "           4     0.8584    0.7968    0.8264       251\n",
      "           5     0.8276    0.8108    0.8191       148\n",
      "           6     0.8760    0.8309    0.8528       272\n",
      "           7     0.6294    0.6667    0.6475       135\n",
      "           8     0.7199    0.7808    0.7491       260\n",
      "           9     0.8268    0.8268    0.8268       254\n",
      "          10     0.7449    0.7572    0.7510       243\n",
      "          11     0.7022    0.6529    0.6767       242\n",
      "          12     0.5447    0.5426    0.5437       258\n",
      "          13     0.6716    0.8405    0.7466       163\n",
      "          14     0.7992    0.7962    0.7977       265\n",
      "          15     0.5282    0.7143    0.6073       105\n",
      "          16     0.8571    0.8790    0.8679       157\n",
      "          17     0.9326    0.9055    0.9188       275\n",
      "          18     0.7810    0.8045    0.7926       266\n",
      "          19     0.7870    0.9048    0.8418       147\n",
      "\n",
      "    accuracy                         0.7459      4486\n",
      "   macro avg     0.7427    0.7534    0.7461      4486\n",
      "weighted avg     0.7480    0.7459    0.7455      4486\n",
      "\n",
      "Confusion Matrix:\n",
      "[[120   0   0  36   0   2   0   5   0   0  24   0  35  21   0  11   3   0\n",
      "    0   1]\n",
      " [  2 191   2   0   4   2   5   1  11   1   5   6   1   0  19   0   2   2\n",
      "   10   5]\n",
      " [  2   5 197   0  12   1   0   0   2   1   0   2   0   0   2   0   0   1\n",
      "    2   0]\n",
      " [ 41   0   0 150   0   1   0   2   0   0  19   0  55   4   0  16   1   0\n",
      "    0   2]\n",
      " [  0   9  17   1 200   0   7   0   7   0   0   1   0   0   4   0   1   0\n",
      "    4   0]\n",
      " [  3   0   0   1   0 120   0   7   0   0   1   0   0   9   0   1   3   0\n",
      "    0   3]\n",
      " [  2   5   2   0   4   0 226   0   3   8   0   2   1   0   6   0   0   2\n",
      "   10   1]\n",
      " [  1   0   0   3   0   7   0  90   0   0   0   0   0  18   0   1   4   0\n",
      "    0  11]\n",
      " [  2   6   1   0   3   0   3   0 203   1   1  29   0   0   6   2   0   2\n",
      "    1   0]\n",
      " [  1   5   1   0   0   1   4   2   5 210   0   2   1   0   3   0   1   4\n",
      "   13   1]\n",
      " [ 11   0   0  15   0   0   0   2   0   0 184   1  12   2   0  13   2   0\n",
      "    0   1]\n",
      " [  0   5   0   3   1   4   6   0  31   4   0 158   0   0   9   1   0   6\n",
      "   13   1]\n",
      " [ 34   0   0  43   0   0   0   6   0   0   9   0 140   7   0  17   2   0\n",
      "    0   0]\n",
      " [  3   0   0   3   0   1   0  12   0   0   1   0   1 137   0   3   1   0\n",
      "    0   1]\n",
      " [  0   3   2   1   6   1   4   1   9   5   1  12   0   0 211   0   1   0\n",
      "    6   2]\n",
      " [  7   0   0   9   0   0   0   1   0   0   2   0   9   1   0  75   0   0\n",
      "    0   1]\n",
      " [  6   0   0   0   0   2   0   4   0   0   0   0   0   2   0   2 138   0\n",
      "    0   3]\n",
      " [  1   1   1   0   2   0   1   0   6   9   0   3   0   0   0   0   0 249\n",
      "    1   1]\n",
      " [  1   2   2   1   1   2   2   3   5  15   0   9   2   0   4   0   0   1\n",
      "  214   2]\n",
      " [  1   0   0   0   0   1   0   7   0   0   0   0   0   3   0   0   2   0\n",
      "    0 133]]\n",
      "Overall Accuracy: 0.7459\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"\\nValidation Set Evaluation (Best Model):\")\n",
    "    print(classification_report(all_labels, all_preds, digits=4))\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(all_labels, all_preds))\n",
    "    print(f\"Overall Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "\n",
    "evaluate_model(best_model, val_loader, DEVICE)\n",
    "\n",
    "\n",
    "def predict_test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    image_ids = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, ids in test_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            predictions.extend(predicted.cpu().numpy())\n",
    "            image_ids.extend(ids) \n",
    "\n",
    "    return image_ids, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80daeaef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T19:21:19.092453Z",
     "iopub.status.busy": "2025-04-30T19:21:19.091779Z",
     "iopub.status.idle": "2025-04-30T19:21:29.885210Z",
     "shell.execute_reply": "2025-04-30T19:21:29.884214Z"
    },
    "papermill": {
     "duration": 10.803402,
     "end_time": "2025-04-30T19:21:29.886778",
     "exception": false,
     "start_time": "2025-04-30T19:21:19.083376",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 4000\n",
      "      id  label\n",
      "0  22430     16\n",
      "1  22431      8\n",
      "2  22432     10\n",
      "3  22433     18\n",
      "4  22434      8\n"
     ]
    }
   ],
   "source": [
    "test_ids, test_preds = predict_test(best_model, test_loader, DEVICE)\n",
    "\n",
    "submission_df = pd.DataFrame({'id': test_ids, 'label': test_preds})\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "print(f\"Number of predictions: {len(submission_df)}\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f248f664",
   "metadata": {
    "papermill": {
     "duration": 0.0066,
     "end_time": "2025-04-30T19:21:29.900279",
     "exception": false,
     "start_time": "2025-04-30T19:21:29.893679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11751646,
     "sourceId": 96834,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7326.542371,
   "end_time": "2025-04-30T19:21:33.221451",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-30T17:19:26.679080",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
